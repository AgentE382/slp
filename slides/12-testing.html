<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>CS 4971: 12-testing slide set</title>
    <meta name="description" content="A set of slides for UVa's Service Learning Practicum course">
    <meta name="author" content="Aaron Bloomfield">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <link rel="stylesheet" href="../slides/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="../slides/reveal.js/css/theme/black.css" id="theme">
    <link rel="stylesheet" href="../slides/css/slp.css">
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="../slides/reveal.js/lib/css/zenburn.css">
    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? '../slides/reveal.js/css/print/pdf.css' : '../slides/reveal.js/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
	<script src="../slides/reveal.js/lib/js/html5shiv.js"></script>
	<![endif]-->
    <script type="text/javascript" src="../slides/js/dhtmlwindow.js"></script>
    <script type="text/javascript" src="../slides/js/canvas.js"></script>
    <link rel="stylesheet" href="../slides/css/dhtmlwindow.css" type="text/css">
  </head>

  <body>
    <div id="dhtmlwindowholder"><span style="display:none"></span></div>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">

	  <section data-markdown><script type="text/template">
# CS 4971
&nbsp;
### Capstone Practicum II
&nbsp;
<center><small>[Aaron Bloomfield](http://www.cs.virginia.edu/~asb) / [aaron@virginia.edu](mailto:aaron@virginia.edu) / [@bloomfieldaaron](http://twitter.com/bloomfieldaaron)</small></center>
<center><small>Repository: [github.com/aaronbloomfield/slp](http://github.com/aaronbloomfield/slp) / [&uarr;](index.html) / <a href="12-testing.html?print-pdf"><img class="print" width="20" src="images/print-icon.png"></a></small></center>
&nbsp;  
&nbsp;
## Testing and the Test Plan
	</script></section>

	<section data-markdown><script type="text/template">
#Contents
&nbsp;  
[Introduction](#/introduction)  
[Requirements](#/requirements)  
[Terminology](#/terminology)  
[Test Types](#/specific)  
[Master Test Plan](#/mastertestplan)  
[Specific Test Plans](#/specificplans)  
[Main Code Coverage](#/codecoverage)  
[Javascript Code Coverage](#/jscodecoverage)  
[Conclusions](#/conclusion)  
	</script></section>

	<section>

	  <section id="introduction" data-markdown><script type="text/template">
# Introduction
	  </script></section>

	  <section data-markdown><script type="text/template">
## Testing is not absolute!
- You can never prove that a (non-trivial) program is bug free
  - It's a corollary of the [halting problem](http://en.wikipedia.org/wiki/Halting_problem): prove that a program halts (successfully) on a given input
  - Alan Turing proved that this was *undecidable* (meaning an algorithm cannot be constructed that could decide this)
- It's very easy to prove that a bug exists, though...
- But you can give yourself -- and your customer -- some sense of confidence that it will *likely* work correctly
	  </script></section>

	  <section data-markdown><script type="text/template">
## Bug rate
- A 1990 review estimated that there were 1 to 3 bugs per 100 executable statements
  - Actually, programmers make many more (1.5 bugs per executable statement!), but they will catch 99% of those
- Your job is to catch the other 1%
	  </script></section>

	  <section data-markdown><script type="text/template">
## Costs of transition
- A much older study indicated that transition and maintenance takes 67% of the cost of a project
  - And that doesn't include an additional 15% of testing during development!
  - Granted, that was during the waterfall methodology craze...
- You are finished with this class when the semester ends
  - If you want to help debug it later on, that's great -- but there is no requirement nor expectation for you to do so
- So we need to ensure that it is well tested before the semester concludes
	  </script></section>

	  <section data-markdown><script type="text/template">
## Things to keep in mind...
- We can *NOT* rely on the customer to complete tests!
- There will be *NO* new code features developed during iterations 12-14
  - Only test cases, bug fixes, minor feature changes requested by the customer, etc.
- Your test plan must be significant, and must have real thought put into it
  - You can start at the [Wikipedia Software Testing portal page](http://en.wikipedia.org/wiki/Portal:Software_Testing)
  - If you wait until the last minute, then throw something half-baked together, I am going to get very cranky
	  </script></section>

	  <section data-markdown><script type="text/template">
## References
- Wikipedia, of course: the [portal on software testing](http://en.wikipedia.org/wiki/Portal:Software_Testing)
- [IEEE 829 (2008) test plan standard](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4578383)
  - Free if viewed from UVa; it's also in Collab as [Resources/misc/IEEE-829-2008.pdf](https://collab.itc.virginia.edu/access/content/group/34417471-f045-46f1-a50f-738fa387fa54/misc/IEEE-829-2008.pdf) as well
  - But see the warnings later in this slide set before you read through this
	  </script></section>

	</section>

	<section>

	  <section id="requirements" data-markdown><script type="text/template">
# Requirements
	  </script></section>

	  <section data-markdown><script type="text/template">
## Why requirements?
- We can't have a discussion about testing without having some set of requirements to test against
- Requirements are categorized into functional and non-functional
- Software attributes are described by the acronym FURPS
  - Functional, Usability, Reliability, Performance, Supportability
	  </script></section>

	  <section data-markdown><script type="text/template">
## Functional & non-functional requirements
- Functional requirements define what a system is supposed to do ("system shall *do* X")
  - Given an input, what is the desired outcome / output?
  - These are *objective* tests and results
- Non-functional define how a system is supposed to be ("system shall *be* X")
  - Rather than specific behaviors, it is criteria used to judge a system
  - These are *subjective* tests and results
- Functional requirements are defined in the system design; non-functional in the system architecture
	  </script></section>

	  <section data-markdown><script type="text/template">
## Types of requirements
- Requirements are usually described by the FURPS acronym
  - F: Funcionality
  - U: Usability
  - R: Reliability
  - P: Performance
  - S: Stability
	  </script></section>

	  <section data-markdown><script type="text/template">
## Furps: Functionality
- Feature set
- Capabilities
- Generality
- Security
	  </script></section>

	  <section data-markdown><script type="text/template">
## fUrps: Usability
- Human factors
- Aesthetics
- Consistency
- Documentation
	  </script></section>

	  <section data-markdown><script type="text/template">
## fuRps: Reliability
- Frequency/severity of failure
- Recoverability
- Predictability
- Accuracy
- Mean time to failure
	  </script></section>

	  <section data-markdown><script type="text/template">
## furPs: Performance
- Speed
- Efficiency
- Resource consumption
- Throughput
- Response time
	  </script></section>

	  <section data-markdown><script type="text/template">
## furpS: Supportability
- Testability
- Extensibility
- Adaptability
- Maintainability
- Compatibility
- Configurability
- Serviceability
- Installability
- Localizability
- Portability
	  </script></section>

	  <section data-markdown><script type="text/template">
## Analyzing requirement solicitation
- You've all seen SIS
  - Which requirements did it have and meet?
  - Which ones did not not have and/or not meet?
- Another example: the R25 room scheduler
	  </script></section>

	  <section data-markdown><script type="text/template">
## Requirements document
- Creating the test plan assumes that you have an updated requirements document
- If there are no requirements (or the requirements are unknown), then there is no point in testing!
	  </script></section>

	</section>

	<section>

	  <section id="terminology" data-markdown><script type="text/template">
# Terminology
	  </script></section>

	  <section data-markdown><script type="text/template">
## What is a software "bug"?
- ... when the system crashes
- ... when the system has undesirable results
- ... when the system does not handle border cases
- ... when the system has invalid output
- ... when the system has calculation errors
- ... when the system has control flow errors
- ... when the system has race conditions
- ... when the system has errors interpreting data
- More [here](http://en.wikipedia.org/wiki/Software_bug#Common_types_of_computer_bugs)
	  </script></section>

	  <section data-markdown><script type="text/template">
## What is a software "defect"?
- Certainly any software bug
- When the software system does not conform to the requirement specification
  - This assumes that the specification exists and is correct!
- Usability problems: too difficult to navigate, etc.
- Missing commands
  - If it's not in the specification, then it's a specification error as well
- Performance issues
- Licensing issues!
  - This is particularly relevant to us
	  </script></section>

	  <section data-markdown><script type="text/template">
## Terminology
- Black box testing: internal design is not considered, only external design
- White box testing: testing is based on detailed knowledge of how the code works
- Regression testing: testing the entire system after a modification has been made
  - We are using Travis/Circle for this
- Acceptance testing: running the tests to make sure the system is in an 'acceptable' state (i.e., has passed all the tests)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Alpha versus beta testing
- Alpha testing: done at the end of development to make sure the system works
  - This is what you will be doing during development but BEFORE the end of iteration 11 (i.e., March 28th)
- Beta testing: an initial version is deployed, and tested "in-place" by the team, the customer, and possibly others
  - This is what will happen in iterations 12-14
	  </script></section>

	</section>

	<section>

	  <section id="specific" data-markdown><script type="text/template">
# Test Types
	  </script></section>

	  <section data-markdown><script type="text/template">
## "On the fly" testing
- Try to do a bunch of things, and see if it doesn't work right
  - This is the typical testing done on may types of projects (undergraduate course projects, short pieces of software, etc.)
- Good as an initial start, but this does not provide very good coverage...
	  </script></section>

	  <section data-markdown id="reqtesting"><script type="text/template">
## System or requirements testing
- Does the system handle the specified requirements?
- Go through each user case / user story, and see if all the parts are included as system functionality
  - If not, then either the system has to be modified or the use case / user story needs to be updated
- Treat the system as a black box
- Once the user manual is written, you will need to ensure that it conforms to the usage of the system
	  </script></section>

	  <section data-markdown><script type="text/template">
## Unit testing
- A set of tests of each individual "unit"
  - Method, procedure, class, etc.
- We have instructions on unit testing for the various platforms, which are good places to start
- The unit testing will need to cover ALL the aspects of the system
- There will be a significant amount of code for this!
	  </script></section>

	  <section data-markdown><script type="text/template">
## Installation testing
- Do the [installation instructions](02-deliverables.html#/installation) work?
- How do we know (meaning, how can we ascertain this)?
	  </script></section>

	  <section data-markdown><script type="text/template">
## Usability testing
- Most of these systems are intended to be used by specific, trained, individuals
  - But not all!
- So while we don't need a perfect UI, it should still be relatively easy to use
- Can the user understand the application flow?  Are the editing functions easy to use?  Is there sufficient help documentation available (likely in the user manual)?
	  </script></section>

	  <section data-markdown><script type="text/template">
## Security testing
- Much of the information in these sites is protected under federal law!
- Can the system be hacked?  Penetrated?  Which pages can be accessed without credentials?
- How does the system handle authorization versus authentication?
	  </script></section>

	  <section data-markdown id="malformed"><script type="text/template">
## Malformed input testing
- How well does the system handle malformed input?
  - For *each* type of input, what happens if the input is invalid?
- Likewise, how well does the system handle incorrect URLs?
  - CakePHP can set the various 'debug' option in core.php to control how this is handled
  - Rails and Django can do something similar
  - But you may need to add a 404 page as well
- This may be included in the unit testing (for malformed input) or system testing (for malformed URLs)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Testing we likely won't be doing...
- Compatibility testing: our system is only going to run on one type of configuration
- Load, stress, and performance testing: one or two groups may need this, but most won't
- Recovery testing: our system is not the type that can "crash" very often
	  </script></section>

	  <section data-markdown><script type="text/template">
## What about other test types?
- If your system requires other types of tests, then you should definitely include them!
	  </script></section>

	</section>


	<section>

	  <section id="mastertestplan" data-markdown><script type="text/template">
# Master Test Plan
	  </script></section>

	  <section data-markdown><script type="text/template">
## Master test plan
- Due at the end of iteration 10 (March 14th)
- This is a high-level overview of the testing
	  </script></section>

	  <section data-markdown><script type="text/template">
## IEEE standard
- [IEEE 829 (2008): Standard for Software and System Test Documentation](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4578383)
  - Free if viewed from UVa; it's also in Collab as [Resources/misc/IEEE-829-2008.pdf](https://collab.itc.virginia.edu/access/content/group/54bcbf62-81cb-45bc-bf8c-c7d46467bba5/misc/IEEE-829-2008.pdf)
  - And on [Wikipedia](http://en.wikipedia.org/wiki/IEEE_829)
- Section 8 concerns the Master Test Plan ("MTP" in the document)
- Some of it is fluff-inducing, so we'll take it with a ***HUGE*** grain of salt
  - Basically, if something doesn't apply, then state so rather than make something up
  - The listing there is meant to provide a structure, not cause you to BS your way through the test plan
	  </script></section>

	  <section data-markdown><script type="text/template">
## Relevant parts of the IEEE standard
- Some sections are where the real content is going
  - Section 8.1.5.6: tools, techniques, methods, and metrics
  - Section 8.2.1: Test processes including definition of test levels
    - (well, less on the test levels...)
- That being said, the following slides are things that should go somewhere in the test plan...
  - And questions to ask yourself -- the goal is for you to *think* about the testing process
	  </script></section>

	  <section data-markdown><script type="text/template">
## What *parts* of the system will be tested?
- Obviously not the framework code itself
- Probably not all the code of the gems (for Rails), packages (for Django), or plugins (for CakePHP) that you are using
  - But likely your code to interact with them
  - And certainly their functionality
- So saying "everything else" is missing the point
  - List the specific methods and functionality that are going to be tested
  - You can certainly use wildcards, etc., for this
- You have MVC components, and you will need to test all those parts
	  </script></section>

	  <section data-markdown><script type="text/template">
## Who will write the tests?
- Is it going to be all one person?  (this is a BAD idea!)
- Will it be a small group within the overall group?
- Will it be everybody?
- If more than one person, who will write the tests for what?
	  </script></section>

	  <section data-markdown><script type="text/template">
## How will you determine what cases to test?
- Corner cases, malformed input cases, first to perform an action, scheduling in a full calendar, etc.
- This will require creating lots of *fixtures*, which are the sample data in the DB when the unit test is run
	  </script></section>

	  <section data-markdown><script type="text/template">
## How will you determine what tests to write?
- There are a lot of sites online for "how to test a X", where 'X' is a various part of your system
  - Model, View, Controller, etc.
- See [here](http://book.cakephp.org/2.0/en/development/testing.html) for CakePHP, [here](http://guides.rubyonrails.org/testing.html) for Rails, and [here](https://docs.djangoproject.com/en/1.6/topics/testing/) for Django
	  </script></section>

	  <section data-markdown><script type="text/template">
## How will you indicate what is being tested?
- From the point of view of the tests, you will need to indicate what requirement is being tested
  - This can be a one-line comment in the unit test
- From the point of view of the requirements, what tests are validating that those requirements work?
	  </script></section>

	  <section data-markdown><script type="text/template">
## What *other* types of tests (other than unit tests) will you be performing?
- Hint: look at the [previous section](#/specific) for details
  - This includes, among other things: installation testing, usability testing, security testing, FURPS, etc.
- You will need to give an overview (following the concepts discussed in the previous few slides) for these tests as well
	  </script></section>

	  <section data-markdown><script type="text/template">
## Remember!
- If something the IEEE docuemnt doesn't make sense, doesn't apply, or is confusing, or seems to be in another language, you can ignore it
- That document is meant to provide *structure*, not to force you to create a lot of fluff
  - I really hate fluff
	  </script></section>

	  <section data-markdown><script type="text/template">
## Tests that are elsewhere
- The master test plan will *briefly* describe other tests to be performed
  - Installation test, usability test, security test, requirements test, etc.
- These will be completed in a separate iteration (that's the [speicifc test plan](#/specifictests))
  - But you need a brief description of each in your MTP; see next slide
	  </script></section>

	  <section data-markdown><script type="text/template">
## MTP contents: the specific tests
- A brief description of the specific test types that you plan on performing
  - What requirement each test (or test type) is validating
  - How you plan on doing those tests.  Some examples:
    - Requirements testing may mean that you go through each of the requirements (which must be linked!) to ensure that they match
    - Security testing will require a specific set of "hacks" against the system, such as X, Y, and Z
    - Unit tests: what parts of the system are you testing?  With which unit tests?  How will you determine which unit tests to write?
- The point of this part is to think of the testing plan from the point of view of the tests that are going to be performed
- No details yet -- that's done later
	  </script></section>

	  <section data-markdown><script type="text/template">
## What should NOT be in the MTP (yet)
- The specific unit tests that you are going to be performing
  - You should give overall ideas, but the specific unit tests are in the source code
- Full details about the specific test plan tests
  - Example: the "installation test" can be a short paragraph stating that the tester(s) will follow the installation manual from an unconfigured machine
    - Since there is no installation manual yet, there is no need to go into more detail at this point
	  </script></section>

	  <section data-markdown><script type="text/template">
## Sample MTP Outline
- Basically, the slides in this section describe the various sections of the MTP
  - Relevant parts of the IEEE MTP document
  - A listing of the parts of the system to be tested
  - A description of who will write the tests
  - How you will determine what cases to test?
  - How will you determine what unit tests to write?
  - How will you indicate what is being tested?
  - What other (non-unit) tests will you be performing?  This is the brief outline of the specific test plan
	  </script></section>

	</section>

	<section>

	  <section data-markdown id="specificplans"><script type="text/template">
# Specific test plans
	  </script></section>

	  <section data-markdown><script type="text/template">
## Specific test plans
- Whereas the master test plan (MTP) is the high-level overview...
- ... we need some specific details about the tests
- These are the specific test plans
  - Installation test (does it install properly?)
  - Usability test (is it usable?)
  - Security testing (is it secure?)
  - Requirements testing (does it meet the requirements?)
  - Compability testing (does it work on multiple browsers?)
  - More, as necessary
	  </script></section>

	  <section data-markdown><script type="text/template">
## Submission requirements
- A brief outline of the specific test plan will be part of the Master Test Plan
- The detailed version (due later) should be combined into a single file (with multiple sections, one for each test type) called `docs/specific-test-plans.md`
- Note that you may not need all of the tests described here
  - If you don't need one, then mention so in the specific-test-plans.md file so we know you intentionally didn't include it (as opposed to you forgot it)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Due dates
- Preliminary versions of these are due with the master test plan (iteration 10 (March 14th))
  - Just a high-level outline so we know which specific test plans you will writing -- you only need to include a paragraph for each specific test plan
  - This is *part* of the MTP
- The full versions of the tests are due at the end of iteration 12 (April 11h)
  - That's one iteration after development ends
  - Some of the tests need not be *compelted* then, even though the test plans are due then
- Completed tests by the end of iteration 13 (April 25th)
  - That's two iterations after development ends
	  </script></section>

	  <section data-markdown><script type="text/template">
## Installation test
- You are testing that the [installation instructions](02-deliverables.html#/installation) work
- This can be a single paragraph stating:
  - What the system is that you will start with
    - Perhaps a VirtualBox image without apache installed
    - You can assume a package manager such as apt-get
  - How you will know if the test works or not
    - You can't just say "if it works" -- that's not a measurable goal
    - Instead, perhaps you know it works because they can do functions A, B, and C on the system, and the unit tests all pass
- You have to actually run through this test!!!
  - Although that is after development ends
	  </script></section>

	  <section data-markdown><script type="text/template">
## Usability test
- This is a *subjective* test, not an *objective* test
  - This means that you will have to get people's *opinions* as to how usable the system is!
- Who to ask?
  - Certainly the customer and the members of your group
  - Members of other groups (you will likely have to reciprocate)
  - Friends / relatives / significant other(s)
- What you want from them is *opinions* of the usability of the system
  - We want honest opinions and feedback, not just "it looks great"
  - They will only need to spend 5 (or so) minutes playing with the system; you can help guide them through
	  </script></section>

	  <section data-markdown><script type="text/template">
## Usability test plan
- It should include:
  - What parts of the system people are going to use
  - What questions you will ask them, and what opinions you want to get from them
- Once you get those, you should put them in the plan (please anonymize)
  - And then use those opinions to improve your system!
- We are looking for at least 4 people *outside* the group, and at least 2 of those who are not the customer or in this class
  - This is in addition to the entire group and the customer
  - And this can be long-hand opinions; we aren't looking for formal human subject experiements with Likert scales here
	  </script></section>

	  <section data-markdown><script type="text/template">
## Security testing, part 1
- Questions to answer:
  - What prevents people from doing actions they should not be allowed to do (deleting things)?
  - What about bot-based password login attempts?
  - [SQL injection](https://en.wikipedia.org/wiki/SQL_injection) and/or [XSS](https://en.wikipedia.org/wiki/Cross-site_scripting) attacks?
  - What about password choice security (i.e., not 'password' as your password)?
  - Malformed input testing, described [earlier](#/malformed)
  - And lots of others...
	  </script></section>

	  <section data-markdown><script type="text/template">
## Security testing, part 2
- These can be a series of unit tests!
  - If so, just briefly state which ones they are -- and why they accomplish this goal
  - Keep in mind that your framework will handle much of the security for you
- You can also describe a test where people try to log in (or whatever), and ensure the result is the desired one
	  </script></section>

	  <section data-markdown><script type="text/template">
## Requirements testing
- Does the system meet all the requirements?
  - As described [before](#/reqtest)
- This will likely be a outline as to how you will go through each requirement and ensure that it is properly implemented in the system
  - One paragraph is propbably sufficient
- You have to actually run through this test!!!
  - Although that is after development ends
	  </script></section>

	  <section data-markdown id="compatabilitytesting"><script type="text/template">
## Compatability Testing
- Here is the entire compatability test plan (it assumes that you have completed the requirements test from the previous slide):
> We will execute the requirements test on the most recent version of the major web browsers (Firefox, Chrome, IE, and Safari).  If -- and only if -- the requirements test passes on all of those browsers, then the compability test will be considered to have passed.
- You can just cut-and-paste this directly
	  </script></section>

	  <section data-markdown><script type="text/template">
## Other test types?
- The specifics of your system may require it
  - Load testing
  - Recovery testing
- Perhaps others...
	  </script></section>

	</section>


	<section>

	  <section id="codecoverage" data-markdown><script type="text/template">
# Main Code Coverage
	  </script></section>

	  <section data-markdown><script type="text/template">
## "Main" Code Coverage
- In this slide set, we will be discussing two types of code coverage:
  - The code coverage of your PHP / Python / Ruby code
    - So named "main" code coverage, which is this section
  - The code coverage of your Javascript client-side code
    - So named "Javascript" code coverage, which is the [next section](#/jscodecoverage)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Problems with code coverage
Consider the following pseudo-code:
```
if ( foo() && bar() && baz() ) {
  // do something...
}
```
- If `foo()` returns false, then by [short circuit evaluation](http://en.wikipedia.org/wiki/Short-circuit_evaluation), `bar()` and `baz()` will not be evaluated
  - PHP, Ruby, and Python all perform short-circuit evaluation (as do most programming languages)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Code coverage is not absolute!
- Code coverage is just one metric (of many)
- Consider the following (purposely terrible) PHP code:
```
class MyClass {
    public function __construct(Array $properties=array()){
      foreach($properties as $key => $value)
        $this->{$key} = $value;
    }
}
function qux($x) {
    $a = array( 0 => array('field'=>0),
                1 => array('field'=>1),
                3 => null);
    $var = new MyClass($a[$x]);
    echo print_r($var,true) . "\n";
    echo $var->field . "\n";
}
```
- What does it do?
- What bugs can we find?
	  </script></section>

	  <section data-markdown><script type="text/template">
## Code coverage is not absolute!
```
function qux($x) {
    $a = array( 0 => array('field'=>0),
                1 => array('field'=>1),
                3 => null);
    $var = new MyClass($a[$x]);
    echo print_r($var,true) . "\n";
    echo $var->field . "\n";
}
```
- For full code coverage, we can just run:
```
qux(0);
qux(1);
```
- But there are *multiple* test cases that we don't test, but will fail (for different reasons):
```
qux(-1);
qux(2);
qux(3);
qux(4);
```
	  </script></section>

	  <section data-markdown><script type="text/template">
## Code coverage is not absolute!
- Two test cases work, most do not
  - But only one of the working ones will generate 100% code coverage (for that function)
- The problem is that we did not put any *thought* into the unit tests
  - We did not test array out of bounds (`qux(-1)` and `qux(4)`)
  - We did not test null pointer exceptions (`qux(3)`)
  - We did not test "holes" in the array (`qux(2)`)
  - We did not test boundary cases (`qux(3)` and `qux(4)`)
- The purpose of writing unit tests is to test for all of these conditions!
  - Put thought into the unit test design!
	  </script></section>

	  <section data-markdown><script type="text/template">
## Code coverage requirements
- Everybody will need to run a code coverage tool
  - Suggestions follow for each platform; you can also choose your own
- The output of said code coverage tool must be included in your repo in a `docs/coverage` directory
  - And updated regularly!
- You will have code coverage percentage requirements!
  - Eventually reaching 100%
  - That's 100% of *your* code, not the framework code
- Each of the platform tools follow, with an online example
	  </script></section>

	  <section data-markdown id="codecoverage-python"><script type="text/template">
## Python / Django instructions
- This assumes that `python manage.py test` works
- Install [coverage.py](https://pypi.python.org/pypi/coverage) (already done on the course server):
```
sudo apt-get install python-coverage
```
  - Note that the binary is `python-coverage`
- The tests must be run *prior* to generating the reports:
```
python-coverage run --source='.' manage.py test
```
- Commands generate the report:
```
python-coverage report
python-coverage report -m
python-coverage html -d coverage
```
  - The first is a text-based report; the second is text-based and includes line numbers; the third is an HTML-based report (in a coverage/ sub-directory)
	  </script></section>

	  <section data-markdown id="codecoverage-php"><script type="text/template">
## PHP / CakePHP instructions
- This assumes that `phpunit` works
- No additional installs are necessary
- The tests are run *while* generating the reports, not before
- Commands to generate the report:
```
phpunit --coverage-text
phpunit --coverage-text=report.txt
phpunit --coverage-html coverage
```
  - The first generates a text-based report to standard output; the second generates a text-based report to `report.txt`; the third creates an HTML-based report in the `coverage/` sub-directory
	  </script></section>

	  <section data-markdown id="codecoverage-rails"><script type="text/template">
## Ruby / Rails
- This assumes that `bundle exec rake test` works
- Installation (instructions are duplicated from [here](https://github.com/colszowka/simplecov#getting-started)):
  - Install the [simplecov](https://github.com/colszowka/simplecov) gem
    - Put in the following into the Gemfile:
```
gem 'simplecov', :require => false, :group => :test
```
    - Run `bundle install --path=vendor/bundle`
  - Add a few lines to the top of `test/test_helper.rb`:
```
if ENV['RAILS_ENV'] == 'test'
      require 'simplecov'
      SimpleCov.start 'rails'
      puts "required simplecov"
end
```
- Now when you run `bundle exec rake test`, it will generate the HTML report in the `coverage/` sub-directory
	  </script></section>

	  <section data-markdown><script type="text/template">
## Links
- Django
  - [coverage.py](https://pypi.python.org/pypi/coverage) and the [documentation](http://nedbatchelder.com/code/coverage/)
  - [Django testing tutorial for version 1.6](https://docs.djangoproject.com/en/1.6/topics/testing/)
    - [The part of that page on code coverage](https://docs.djangoproject.com/en/1.6/topics/testing/advanced/#integration-with-coverage-py)
- CakePHP
  - [phpunit](https://phpunit.de/) and the [documentation](https://phpunit.de/manual/current/en/index.html)
  - [CakePHP's testing page](http://book.cakephp.org/3.0/en/development/testing.html)
    - [The part of that page on code coverage](http://book.cakephp.org/3.0/en/development/testing.html#generating-code-coverage)
- Rails
  - [Rails testing page](http://guides.rubyonrails.org/testing.html)
  - [simplecov](https://github.com/colszowka/simplecov)
    - [The part of that page on installation](https://github.com/colszowka/simplecov#getting-started)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Using other tools
- The tools listed here were chosen because they can run on the course server and on your VirtualBox image, and do not require external accounts
- You are welcome to use a different code coverage tool, with the following requirements:
  - You need to run it by me first
  - It needs to generate the same type of data:
    - Percent covered for the entire system
    - Percent covered for each individual "part" (file, class, etc.)
    - Specific parts not covered (displayed code with line numbers, most likely)
    - All this in a nicely formatted HTML-based report (with hyperlinks and what-not)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Submission requirements
- Each of the commands listed previous create a `coverage/` sub-directory
  - If you use another tool, then an HTML-based copy of the report should likewise be put into a `coverage/` sub-directory
- That `coverage/` sub-directory must go in the `docs/` directory in your repo (i.e., `docs/coverage/`)
  - The assumption is that there is a `docs/coverage/index.html` file used to view the report
  - And viewing that file will properly view the other HTML-based assets (images, CSS, etc.)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Code Coverage Amount
- But the time development ends at the end of iteration 12 (one week into the transition phase), your unit tests must have 100% coverage of your code!
- By the end of iteration 11 (when development ends), we are looking for 80% coverage
  - 60% coverage by the end of iteration 9
  - 70% coverage by the end of iteration 10
- All of these are *overall* coverage; coverage for individual files may vary (except for the 100% one, of course)
	  </script></section>

	  <section data-markdown id="codecoverageslacking"><script type="text/template">
## If <100% code coverage...
- It may be that there is a part of your code that you cannot test, such as from a sanity check
```
if ( the sun explodes )
    print "sorry, I cannot continue"
```
  - If there is such code in your system, it will prohibit you from hitting 100% code coverage
- In this case, create a docs/code-coverage-excuses.md file that explains for ***each*** code snippet that is not covered:
  - What code it is that is not covered
  - Why it is not covered
  - Why you think it is reasonable that you code will still work
- This is only for code that cannot be tested!
	  </script></section>

	</section>


	<section>

	  <section id="jscodecoverage" data-markdown><script type="text/template">
# Javascript Code Coverage
	  </script></section>

	  <section data-markdown><script type="text/template">
## Javascript Code Coverage
- You typically have to run it in a browser, which is more difficult than it should be
- A full tutorial of this setup procedure is in the [uva-slp/tutorials](https://github.com/uva-slp/tutorials) repo, specifically [here](https://github.com/uva-slp/tutorials/blob/master/other/qunit-blanket-phantomjs/index.md)
- There are three parts:
  - [QUnit](http://qunitjs.com), which is the unit testing framework
  - [blanket.js](http://blanketjs.org/), which is the code coverage tool
  - [PhantomJS](http://phantomjs.org/), which is a headless browser that can automate the tests
- The result of this will be a jscoverage.pdf file, which goes in the docs/ directory in your repo
	  </script></section>

	  <section data-markdown><script type="text/template">
## Part 1: Set up unit testing
- Your Javascript code should all be in separate .js files
  - And not embedded in your HTML pages
- You are going to want to use [QUnit](http://qunitjs.com)
  - It works really well with Javascript
- You will create a qunit.html file, based on the example at [QUnit](http://qunitjs.com), which will include all your system's Javascript files
- A sample unit test:
```
QUnit.test( "hello test", function( assert ) {
    assert.ok( 1 == "1", "Passed!" );
});
```
- Then write some (real) unit tests!
	  </script></section>

	  <section data-markdown><script type="text/template">
## Part 2: Set up the code coverage tool
- We are using [blanket.js](http://blanketjs.org/)
  - This is recommended by QUnit, and integrates well with it
- You have to download the blanket.min.js file, and put that in a `<script>` line in your qunit.html file
  - But AFTER you include the qunit.js file!
- When you re-run the tests, an "Enable coverage" check box will appear
	  </script></section>

	  <section data-markdown><script type="text/template">
## Part 3: Set up PhantomJS
- [PhantomJS](http://phantomjs.org/) is a headless browser, which allows you to run the tests, and save the output
- First, install it: `sudo apt-get install phantomjs`
- Next, download the [run-qunit.js](https://github.com/ariya/phantomjs/blob/master/examples/run-qunit.js) script
- Modify that script, as described in the [tutorial](https://github.com/uva-slp/tutorials/blob/master/other/qunit-blanket-phantomjs/index.md)
- The command to run it is:
```
phantomjs run-qunit.js file://`pwd`/qunit.html\?coverage
```
  - Note the backslash!
- This will output a jscoverage.pdf file, which will contain a report of the code coverage
	  </script></section>

	  <section data-markdown><script type="text/template">
## Result
This is what the jscoverage.pdf file (and thus the qunit.html web page) looks like:
![](images/12-testing/blanket.js-output.png)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Javascript Code Coverage Schedule
- Iteration 10: 25% of your Javascript code needs to be covered by unit tests
- Iteration 11: 50% of your Javascript code needs to be covered by unit tests
- Iteration 12: 75% of your Javascript code needs to be covered by unit tests
- Iteration 13: 100% of your Javascript code needs to be covered by unit tests
	  </script></section>

	</section>


	<section>

	  <section id="conclusion" data-markdown><script type="text/template">
# Conclusions
	  </script></section>

	  <section data-markdown><script type="text/template">
## One other requirement...
- Your unit tests, in addition to running on Travis or Circle, must *also* be able to be run on the course server!
  - A formal requirement for iteration 12
	  </script></section>

	  <section data-markdown><script type="text/template">
## Acceptance testing
- This is a test (or multiple tests) performed to ensure that the system meets customer specifications
- For our purposes, the 'acceptance test' will be the successful execution of the master test plan
	  </script></section>

	  <section data-markdown><script type="text/template">
## Iteration deliverables, part 1
- This is all detailed in the [iteration goals](../uva/iteration-goals.html#/) slide set, and will be mentioned in the [daily announcements](../uva/daily-announcements.html#/)
  - All of these are due on the last day (a Monday) of each iteration
- Iteration 7: deployment plan (due Feb 1st)
- Iteration 8: installation instructions (due Feb 15th)
- Iteration 9: main code coverage @ 60% (due Feb 29th)
- Iteration 10: master test plan (due Mar 14th)
  - Which includes the brief summary of the specific test plans
  - As well as main code coverage @ 70%
  - And Javascript code coverage @ 25%
	  </script></section>

	  <section data-markdown><script type="text/template">
## Iteration deliverables, part 2
- Iteration 11: the full transition plan, encompassing all of the above, all edited as necessary (due Mar 28th)
  - As well as main code coverage @ 80%
  - And Javascript code coverage @ 50%
- Iteration 12: the completed specific test plans (due Apr 11th)
  - As well as main code coverage @ 100%
  - And Javascript code coverage @ 75%
- Iteration 13: all test completed (due Apr 25th)
  - Including the specific test plans
  - And Javascript code coverage @ 100%
- Iteration 14: is project wrap-up, and all testing should be completed before then
	  </script></section>

	  <section data-markdown><script type="text/template">
## Test plan standard
- [IEEE 829 (2008): Standard for Software and System Test Documentation](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4578383)
  - Free if viewed from UVa; it's also in Collab under misc/ as well
  - And on [Wikipedia](http://en.wikipedia.org/wiki/IEEE_829)
- We will be following *SOME THINGS* of this standard
  - The full standard is way too bloated and just encourages fluff
  - This is intended to give you a structure, not to restrict what you do...
	  </script></section>

	  <section data-markdown><script type="text/template">
## I don't want fluff!
- If you add verbiage for the sake of adding verbiage, I'll just get really cranky
  - It's a waste of everybody's time
  - If a section doesn't apply just put a single sentence that this doesn't apply (and perhaps why not)
- That being said, I want a well thought-out master test plan
  - So you are going to have to actually read through some of that standard, various Wikipedia pages, etc.
  - You can start at [Wikipedia](http://en.wikipedia.org/wiki/Portal:Software_Testing)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Iteration deliverables format
- It should be in either Markdown, and kept in the docs/ directory of your github repo
- Name them the following, please:
  - docs/master-test-plan.md
  - docs/specific-test-plans.md
  - docs/coverage/
  - etc.
	  </script></section>

	</section>
	
      </div>

    </div>

    <script src="../slides/reveal.js/lib/js/head.min.js"></script>
    <script src="../slides/reveal.js/js/reveal.js"></script>
    <script src="../slides/js/settings.js"></script>

  </body>
</html>
